{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_libs import *\n",
    "from ds_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train, Validation & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/Users/joshuaighalo/Documents/GitHub/eegDementia/MLOps/dementiaStages/dataframes/\"\n",
    "train = pd.read_csv(dir + \"train_10.csv\")\n",
    "test_6m = pd.read_csv(dir + \"test_6m_10.csv\")\n",
    "test_12m = pd.read_csv(dir + \"test_12m_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train.columns.values.tolist()\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_test_6m = test_6m['filenames'].values.tolist()\n",
    "filenames_test_12m = test_12m['filenames'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_6m = test_6m.drop(['filenames'], axis=1)\n",
    "test_12m = test_12m.drop(['filenames'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Categorical Classes with Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.replace(\"ND\", 0)\n",
    "train = train.replace(\"MMD\", 1)\n",
    "train = train.replace(\"SD\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check & Replace Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(train.mean(), inplace=True)\n",
    "test_6m.fillna(test_6m.mean(), inplace=True)\n",
    "test_12m.fillna(test_12m.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Class\", data=train, palette=sns.xkcd_palette([\"azure\", \"light red\",\"light green\"]))\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=0)\n",
    "xtrain, ytrain = sm.fit_resample(train[features_train[:-1]], train['Class'])\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=ytrain, palette=sns.xkcd_palette([\"azure\", \"light red\",\"light green\"]))\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate Features & Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest_6m,xtest_12m = test_6m, test_12m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "xtrain_norm = scaler.fit_transform(xtrain)\n",
    "xtest_6m_norm = scaler.transform(xtest_6m)\n",
    "xtest_12m_norm = scaler.transform(xtest_12m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "xtrain_std = scaler.fit_transform(xtrain)\n",
    "xtest_6m_std = scaler.transform(xtest_6m)\n",
    "xtest_12m_std = scaler.transform(xtest_12m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99,random_state=0)\n",
    "xtrain_pca = pca.fit_transform(xtrain_norm)\n",
    "xtest_6m_pca = pca.transform(xtest_6m_norm)\n",
    "xtest_12m_pca = pca.transform(xtest_12m_norm)\n",
    "print(\"No. PCA features train: \", xtrain_pca.shape[1])\n",
    "print(\"No. PCA features test_6m: \", xtest_6m_pca.shape[1])\n",
    "print(\"No. PCA features test_12m: \", xtest_12m_pca.shape[1])\n",
    "indices = np.argsort(pca.explained_variance_ratio_)[::-1]\n",
    "names = [features_train[i] for i in indices]\n",
    "names.pop(0)\n",
    "print(\"PCA features:\",*names, sep = \", \")\n",
    "\n",
    "# scatter plot of all principal components seperating the classes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20,7))\n",
    "ax.scatter(xtrain_pca[:, 0], xtrain_pca[:, 1], c=ytrain, cmap='rainbow', alpha=0.5, edgecolors='b')\n",
    "ax.set_xlabel('First Principal Component')\n",
    "ax.set_ylabel('Second Principal Component')\n",
    "ax.set_title('PCA of EEG data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "xtrain_lda = lda.fit_transform(xtrain_std, ytrain)\n",
    "xtest_6m_lda = lda.transform(xtest_6m_std)\n",
    "xtest_12m_lda = lda.transform(xtest_12m_std)\n",
    "print(\"No. LDA features train: \", xtrain_lda.shape[1])\n",
    "print(\"No. LDA features test_6m: \", xtest_6m_lda.shape[1])\n",
    "print(\"No. LDA features test_12m: \", xtest_12m_lda.shape[1])\n",
    "\n",
    "# extract names of the features\n",
    "indices = np.argsort(lda.explained_variance_ratio_)\n",
    "names = [features_train[i] for i in indices]\n",
    "names.pop(0)\n",
    "print(\"LDA features:\",*names, sep = \", \")\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for lab, col in zip((0, 1, 2),\n",
    "                        ('blue', 'red', 'green')):\n",
    "        plt.scatter(xtrain_lda[ytrain == lab, 0],\n",
    "                    xtrain_lda[ytrain == lab, 1],\n",
    "                    label=lab,\n",
    "                    c=col)\n",
    "    plt.xlabel('Linear Discriminant 1')\n",
    "    plt.ylabel('Linear Discriminant 2')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.title('Train')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'gamma': [0, 0.25, 0.5, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.25, 0.5, 0.75, 1.0],\n",
    "    'reg_lambda': [0, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "xgb = XGBClassifier(objective='multi:softmax', num_class=3, random_state=0)\n",
    "args = {'n_iter': 100, 'cv': 3, 'verbose': 0, 'random_state': 0, 'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(xgb, param_distributions=param_grid, **args)\n",
    "random_search.fit(xtrain, ytrain)\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "print(\"Best estimator: \", random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_norm = RandomizedSearchCV(xgb, param_distributions=param_grid, **args)\n",
    "random_search_norm.fit(xtrain_norm, ytrain)\n",
    "print(\"Best parameters: \", random_search_norm.best_params_)\n",
    "print(\"Best score: \", random_search_norm.best_score_)\n",
    "print(\"Best estimator: \", random_search_norm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_std = RandomizedSearchCV(xgb, param_distributions=param_grid, **args)\n",
    "random_search_std.fit(xtrain_std, ytrain)\n",
    "print(\"Best parameters: \", random_search_std.best_params_)\n",
    "print(\"Best score: \", random_search_std.best_score_)\n",
    "print(\"Best estimator: \", random_search_std.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_pca = RandomizedSearchCV(xgb, param_distributions=param_grid, **args)\n",
    "random_search_pca.fit(xtrain_pca, ytrain)\n",
    "print(\"Best parameters: \", random_search_pca.best_params_)\n",
    "print(\"Best score: \", random_search_pca.best_score_)\n",
    "print(\"Best estimator: \", random_search_pca.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_lda = RandomizedSearchCV(xgb, param_distributions=param_grid, **args)\n",
    "random_search_lda.fit(xtrain_lda, ytrain)\n",
    "print(\"Best parameters: \", random_search_lda.best_params_)\n",
    "print(\"Best score: \", random_search_lda.best_score_)\n",
    "print(\"Best estimator: \", random_search_lda.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = random_search.best_estimator_\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(xtrain)\n",
    "scores = cross_val_score(model, xtrain, ytrain, cv=loocv, scoring='accuracy').mean()\n",
    "print(\"LOOCV score: \", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_norm = random_search_norm.best_estimator_\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(xtrain_norm)\n",
    "scores = cross_val_score(model_norm, xtrain_norm, ytrain, cv=loocv, scoring='accuracy').mean()\n",
    "print(\"LOOCV score: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_std = random_search_std.best_estimator_\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(xtrain_std)\n",
    "scores = cross_val_score(model_std, xtrain_std, ytrain, cv=loocv, scoring='accuracy').mean()\n",
    "print(\"LOOCV score: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = random_search_pca.best_estimator_\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(xtrain_pca)\n",
    "scores = cross_val_score(model_pca, xtrain_pca, ytrain, cv=loocv, scoring='accuracy').mean()\n",
    "print(\"LOOCV score: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = random_search_lda.best_estimator_\n",
    "loocv = LeaveOneOut()\n",
    "loocv.get_n_splits(xtrain_lda)\n",
    "scores = cross_val_score(model_lda, xtrain_lda, ytrain, cv=loocv, scoring='accuracy').mean()\n",
    "print(\"LOOCV score: \", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n",
    "rf_pca = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n",
    "rf_norm = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n",
    "rf_std = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n",
    "rf_lda = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n",
    "rf.fit(xtrain, ytrain)\n",
    "rf_pca.fit(xtrain_pca, ytrain)\n",
    "rf_norm.fit(xtrain_norm, ytrain)\n",
    "rf_std.fit(xtrain_std, ytrain)\n",
    "rf_lda.fit(xtrain_lda, ytrain)\n",
    "print(\"Generalization Score:\",rf.oob_score_)\n",
    "print(\"Generalization Score PCA:\",rf_pca.oob_score_)\n",
    "print(\"Generalization Score Normalized:\",rf_norm.oob_score_)\n",
    "print(\"Generalization Score Standardized:\",rf_std.oob_score_)\n",
    "print(\"Generalization Score LDA:\",rf_lda.oob_score_)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.fit(xtrain, ytrain)\n",
    "ypred_6m = model.predict(xtest_6m)\n",
    "ypred_12m = model.predict(xtest_12m)\n",
    "\n",
    "model_norm.fit(xtrain_norm, ytrain)\n",
    "ypred_6m_norm = model_norm.predict(xtest_6m_norm)\n",
    "ypred_12m_norm = model_norm.predict(xtest_12m_norm)\n",
    "\n",
    "model_std.fit(xtrain_std, ytrain)\n",
    "ypred_6m_std = model_std.predict(xtest_6m_std)\n",
    "ypred_12m_std = model_std.predict(xtest_12m_std)\n",
    "\n",
    "model_pca.fit(xtrain_pca, ytrain)\n",
    "ypred_6m_pca = model_pca.predict(xtest_6m_pca)\n",
    "ypred_12m_pca = model_pca.predict(xtest_12m_pca)\n",
    "\n",
    "model_lda.fit(xtrain_lda, ytrain)\n",
    "ypred_6m_lda = model_lda.predict(xtest_6m_lda)\n",
    "ypred_12m_lda = model_lda.predict(xtest_12m_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_0 = np.where(ypred_6m_lda == 0)[0]\n",
    "indices_1 = np.where(ypred_6m_lda == 1)[0]\n",
    "indices_2 = np.where(ypred_6m_lda == 2)[0]\n",
    "pred_ND_6m = np.array(filenames_test_6m)[indices_0]\n",
    "pred_MMD_6m = np.array(filenames_test_6m)[indices_1]\n",
    "pred_SD_6m = np.array(filenames_test_6m)[indices_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_0 = np.where(ypred_12m_lda == 0)[0]\n",
    "indices_1 = np.where(ypred_12m_lda == 1)[0]\n",
    "indices_2 = np.where(ypred_12m_lda == 2)[0]\n",
    "pred_ND_12m = np.array(filenames_test_12m)[indices_0]\n",
    "pred_MMD_12m = np.array(filenames_test_12m)[indices_1]\n",
    "pred_SD_12m = np.array(filenames_test_12m)[indices_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth: ERPs Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '/Users/joshuaighalo/Downloads/brainNet_datasets/laurel_place/cleaned_dataset/'\n",
    "args = {'deviceVersion':1.0,'path':dataPath,'sfreq':cfg.fs,'line':cfg.line,'highPass':1,'lowPass':10,'stimTriggers':cfg.stimTrig,'clip':75,'channel_names':['Fz','Cz','Pz'],'ERPs_GrandAverages':True,'erp_plots':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Dementia | 6 Months | Run 1')\n",
    "erps6ND_1 = pipeline(filenames=pred_ND_6m,**args)\n",
    "\n",
    "print('Mild Dementia | 6 Months | Run 1')\n",
    "erps6MIDMOD_1 = pipeline(filenames=pred_MMD_6m,**args)\n",
    "\n",
    "print('Severe Dementia | 6 Months | Run 1')\n",
    "erps6SD_1 = pipeline(filenames=pred_SD_6m,**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No Dementia | 12 Months | Run 1')\n",
    "erps12ND_1 = pipeline(filenames=pred_ND_12m,**args)\n",
    "\n",
    "print('Mild Dementia | 12 Months | Run 1')\n",
    "erps12MIDMOD_1 = pipeline(filenames=pred_MMD_12m,**args)\n",
    "\n",
    "print('Severe Dementia | 12 Months | Run 1')\n",
    "erps12SD_1 = pipeline(filenames=pred_SD_12m,**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
